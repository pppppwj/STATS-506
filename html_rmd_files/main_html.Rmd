---
title: "STATS 506 - Group Project"
author: "Group 1 - Weijie Pan, Jingxian Chen, Eric Hernandez-Montenegro"
date: "12/9/2019"
output: 
  html_document: 
    theme: journal
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(SASxport)
library(tidyverse)
library(doParallel)
library(glmnet)
library(ROCit)
```

# Introduction

Diabetes is a disease that occurs when your body doesn't make or use the hormone insulin properly. In the past, doctors thought that only adults were at risk of developing type 2 diabetes. However, an increasing number of children in the United States are now being diagnosed with the disease. Doctors think this inrease is mostly because more children's eating habbits are unhealthy. And also, as people paying more attention to their body health nowadays, it is interesting to consider the effect of health insurance on people's eating habbits.
Therefore, we purposed a question relevant to both eatting habbits, health insurance and diabetes situations. Our question is:

**Do people's eating habbits have the same effect on their diabetes status with or without health insurance?**

To answer this question, we decide to first consider a bunch of diet elements in our analysis, and then use a penalized logistic model to help us select the important ones. In order to see whether the eating habbits' effect is different after considering the insurance, we first create new dietary variables interacted with insurance and then implement in our model.   

# Data

The data used is from the National Health and Nutrition Examination Survey (NHANES). The specific datasets used are: 2015-2016 Demographic Variables and Sample Weights, 2015-2016 Health Insurance, 2015-2016 Diabetes, 2015-2016 Dietary Interview - Total Nutrient Intakes (First Day), and 2015-2016 Dietary Interview - Total Nutrient Intakes (Second Day). The following variables were used from the 2015-2016 Demographic Variables and Sample Weights dataset: respondent sequence number (seqn), age, gender, and income. From the 2015-2016 Health Insurance variables used were: respondent sequence number (seqn) and insurance (whether respondent was covered insurance or not). From the 2015-2016 Diabetes dataset, we selected the following variables: respondent sequence number (seqn) and doctor telling respondent if they had diabetes. Lastly, variables that were chosen from the 2015-2016 Dietary Interviews - Total Nutrient Intakes (First and Second Day) were:
Iron, Calcium, Zinc, Sodium, Vitamin E, Vitamin A, Alcohol, Vitamin C, Total fat, Dietary, fiber, Total sugars, Carbohydrate, Energy, and Protein. Full data is available [here](https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx).


# Methods
**Logistic Regression with Lasso Penalty**

Diabete is a binary variable. For binary classification problems, linear logistic regression model is most often used. 
Although logistic regression often performs comparably to competing methods, such as support vector machine and linear 
discriminant analysis, it is chosen in this problem because of its several advantages. The notable advantages include 
that: it provides a direct estimate of class probability; it tends to be more robust in the case $k>>n$
since it makes no assumptions on the distribution of the predictors; and it doesn't need tuning parameters. The logistic regression model presents the class-conditional probabilities through a linear function of the predictors and is expressed as:


$$log(\frac{p}{1-p})=\beta_0+\sum_{i=1}^k{x_i\beta_i}$$


Logistic regression coefficients are typically estimated by maximizing the following binomial log-likelihood:

$$\max_{\beta}\sum_{i=1}^n{\{y_i(x_i^T\beta)-\log(1+\exp(x_i^T\beta))\}}$$


In order to select the variables and avoid overfitting, here we try to use logistic regression with lasso penalty.
Now it becomes to be estimated by minimizing the following formula:
$$\min_{\beta}\{-\frac{1}{n}\sum_{i=1}^n{\{y_i(x_i^T\beta)-\log(1+\exp(x_i^T\beta))\}}+P_\lambda(\beta)\},P_\lambda(\beta)=\lambda\sum_{j=1}^k|\beta_j|$$

**ROC curve and AUC**

Because the data is unbalance. So we use AUC to judge our model.

|  | P(true) |  N(true)|
| ---- | ---- | ----- |
| P'(predicted) |TP|FP| 
| N'(predicted) |FN |TN | 


ROC curve: Y-axis is TPR=TP/(TP+FN) , X-axis is FPR=FP/(FP+TN).   
AUC is value of area under ROC curve.  
Several equivalent interpretations of AUC:   
* The expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.   
* The expected proportion of positives ranked before a uniformly drawn random negative.   
* The expected true positive rate if the ranking is split just before a uniformly drawn random negative.   
* The expected proportion of negatives ranked after a uniformly drawn random positive.   
* 1 â€“ the expected false positive rate if the ranking is split just after a uniformly drawn random positive.   

# Core Analysis {.tabset .tabset-fade .tabset-pills}

## R

### Load data
```{r load_data}
Demographic_data <- read.xport("C:\\Users\\95260\\Desktop\\stats506\\group_project\\DEMO_I.XPT")
Diabetes_data <- read.xport("C:\\Users\\95260\\Desktop\\stats506\\group_project\\DIQ_I.XPT")
Day1_data <- read.xport("C:\\Users\\95260\\Desktop\\stats506\\group_project\\DR1TOT_I.XPT")
Day2_data <- read.xport("C:\\Users\\95260\\Desktop\\stats506\\group_project\\DR2TOT_I.XPT")
Insurance_data <- read.xport("C:\\Users\\95260\\Desktop\\stats506\\group_project\\HIQ_I.XPT")
```

---

### Data Cleaning
For the data cleaning part , I decide to use the dplyr package to help me solve it.
```{r data_clean}
# Demographic data
New_Demo_dat <-  Demographic_data %>%
                    select("id" = SEQN, 
                            "gender" = RIAGENDR, 
                            "age" = RIDAGEYR,
                            "income" = INDFMIN2) %>%
                    filter(!is.na(income) & income != 12 & income != 13 
                           & income != 77 & income != 99) %>%
                    transmute(id = id,
                              gender = factor(gender, labels =c("male","female")),
                              age_group = factor(ifelse(age < 13, 1, 
                                                        ifelse(age < 19,2,
                                                        ifelse(age < 41, 3,
                                                        ifelse(age < 60, 4, 5)))),
                                                 labels = c("younger", "teenager",
                                                            "adult", "seniors", "elder")),
                              income = factor(ifelse(income == 14 | income ==15, income - 2,
                                              income)))

# Insurance data
# remove values other than 1 or 2 and divide insurance variable into 2 columns
New_Insur_dat <- Insurance_data %>%
                    select("id" = SEQN, "insurance" = HIQ011) %>%
                    filter(insurance != 7 & insurance != 9) %>%
                    transmute(id = id,
                              insurance = ifelse(insurance == 1, 1, 0))

# diabetes data
# remove values other than 1 or 2, and change 2 to 0
New_Diabetes_dat <- Diabetes_data %>%
                        select("id" = SEQN, "diabetes" = DIQ010)%>%
                        filter(!is.na(diabetes) & diabetes != 3 & diabetes != 9) %>%
                        transmute(id = id,
                                  diabetes = ifelse(diabetes == 2, 0, 1))

## Day1 data
New_Day1_dat <- Day1_data %>%
                    select("id" = SEQN, "weight" = WTDRD1, "iron" = DR1TIRON,
                           "calcium" = DR1TCALC, "zinc" = DR1TZINC, "sodium" = DR1TSODI,
                           "VE" = DR1TATOC, "VA" = DR1TVARA, "VC" = DR1TVC,
                           "alcohol" = DR1TALCO, "fat" = DR1TTFAT, "fiber" = DR1TFIBE,
                           "sugar" = DR1TSUGR, "carbonhydrate" = DR1TCARB,
                           "energy" = DR1TKCAL, "protein" = DR1TPROT) %>%
                    filter(weight != 0) %>%
                    filter_at(vars(-c("id", "weight")), all_vars(!is.na(.))) %>%
                    # for data visualization part, we don't standaardize the variables
                    mutate_at(vars(-c("id", "weight")), scale)  %>%
                    mutate(survey_day = factor(1))

# Day2 data
New_Day2_dat <- Day2_data %>%
                    select("id" = SEQN, "weight" = WTDR2D, "iron" = DR2TIRON,
                           "calcium" = DR2TCALC, "zinc" = DR2TZINC, "sodium" = DR2TSODI,
                           "VE" = DR2TATOC, "VA" = DR2TVARA, "VC" = DR2TVC,
                           "alcohol" = DR2TALCO, "fat" = DR2TTFAT, "fiber" = DR2TFIBE,
                           "sugar" = DR2TSUGR, "carbonhydrate" = DR2TCARB,
                           "energy" = DR2TKCAL, "protein" = DR2TPROT) %>%
                    filter(weight != 0) %>%
                    filter_all(all_vars(!is.na(.))) %>%
                    # for data visualization part, we don't standaardize the variables
                    mutate_at(vars(-c("id", "weight")), scale) %>%
                    mutate(survey_day = factor(2))

# Merge all dataset together
Data_final <- New_Day1_dat %>%
                rbind(New_Day2_dat) %>% 
                    inner_join(New_Demo_dat, by = "id") %>%
                        inner_join(New_Diabetes_dat, by = "id") %>%
                            inner_join(New_Insur_dat, by = "id")
dim(Data_final)
```
So after cleaning process, I finally got a data with 13245 observations and 22 variables.

```{r show_data, echo=FALSE}
knitr::kable(Data_final[1:3,1:10], caption = "The head 3 rows and head 10 columns of final data ")
```

---

### Data Visualization
For better understanding the final data we got, we decide to do some visualization jobs, here I am going to plot the difference of average zinc iron and sodium intake amount between diabetes and non-diabetes groups at each level of people.
By ploting these data, we can directly understand the eating habbit's difference in microelements intake of people.  

```{r data_visualize, echo=FALSE}
New_Day1_dat <- Day1_data %>%
                    select("id" = SEQN, "weight" = WTDRD1, "iron" = DR1TIRON,
                           "calcium" = DR1TCALC, "zinc" = DR1TZINC, "sodium" = DR1TSODI,
                           "VE" = DR1TATOC, "VA" = DR1TVARA, "VC" = DR1TVC,
                           "alcohol" = DR1TALCO, "fat" = DR1TTFAT, "fiber" = DR1TFIBE,
                           "sugar" = DR1TSUGR, "carbonhydrate" = DR1TCARB,
                           "energy" = DR1TKCAL, "protein" = DR1TPROT) %>%
                    filter(weight != 0) %>%
                    filter_at(vars(-c("id", "weight")), all_vars(!is.na(.))) %>%
                    # for data visualization part, we don't standaardize the variables
                    #mutate_at(vars(-c("id", "weight")), scale)  %>%
                    mutate(survey_day = factor(1))

# Day2 data
New_Day2_dat <- Day2_data %>%
                    select("id" = SEQN, "weight" = WTDR2D, "iron" = DR2TIRON,
                           "calcium" = DR2TCALC, "zinc" = DR2TZINC, "sodium" = DR2TSODI,
                           "VE" = DR2TATOC, "VA" = DR2TVARA, "VC" = DR2TVC,
                           "alcohol" = DR2TALCO, "fat" = DR2TTFAT, "fiber" = DR2TFIBE,
                           "sugar" = DR2TSUGR, "carbonhydrate" = DR2TCARB,
                           "energy" = DR2TKCAL, "protein" = DR2TPROT) %>%
                    filter(weight != 0) %>%
                    filter_all(all_vars(!is.na(.))) %>%
                    # for data visualization part, we don't standaardize the variables
                    #mutate_at(vars(-c("id", "weight")), scale) %>%
                    mutate(survey_day = factor(2))

# Merge all dataset together
Data_final1 <- New_Day1_dat %>%
                rbind(New_Day2_dat) %>% 
                    inner_join(New_Demo_dat, by = "id") %>%
                        inner_join(New_Diabetes_dat, by = "id") %>%
                            inner_join(New_Insur_dat, by = "id")

## Compare the average microelement intake amount at each age, gender and 
## insurance level between day1 and day2.
data_nodiab <- Data_final1 %>%
    filter(diabetes == 0 & survey_day == 1) %>%
    select(c("weight", "age_group", "gender", "insurance","zinc",
             "iron", "sodium")) %>%
    group_by(age_group, gender, insurance) %>%
    summarise(avg_iron = sum(iron * weight) / sum(weight),
              avg_zinc = sum(zinc * weight) / sum(weight),
              avg_sodium = sum(sodium * weight) / sum(weight))

data_diab <- Data_final1 %>%
    filter(diabetes == 1 & survey_day == 1) %>%
    select(c("weight", "age_group", "gender", "insurance","zinc",
             "iron", "sodium")) %>%
    group_by(age_group, gender, insurance) %>%
    summarise(avg_iron = sum(iron * weight) / sum(weight),
              avg_zinc = sum(zinc * weight) / sum(weight),
              avg_sodium = sum(sodium * weight) / sum(weight))

data_combined <- data_nodiab %>%
    inner_join(data_diab, by = c("age_group", "gender", "insurance"),
               suffix = c("_nodb", "_db")) %>%
    pivot_longer(cols = -c("age_group", "gender", "insurance"),
                 names_to = c(".value", "diabetes"), 
                 names_pattern = "(avg_.*[_$])(db|nodb)") %>%
    mutate(group_id = paste(paste(age_group, substr(gender,1,1),sep = "_"), 
                            insurance, sep = "_"))


fig_zinc <- ggplot(data_combined, aes(x = group_id, y = avg_zinc_, fill = diabetes, 
                                      shape = diabetes, color = diabetes)) +
    geom_bar(position = "dodge", stat = "identity") +
    geom_text(aes(label=round(avg_zinc_,1)), color = "black", 
              position = position_dodge(1), size=4) + coord_polar() +
    labs(y = "zinc amount(mg)", title = "Zinc(mg)", 
         subtitle = "Zinc intake amount(mg) among each group of people")

fig_iron <- ggplot(data_combined, aes(x = group_id, y = avg_iron_, fill = diabetes, 
                                      shape = diabetes, color = diabetes)) +
    geom_bar(position = "dodge", stat = "identity") +
    geom_text(aes(label=round(avg_iron_,1)), color = "black", 
              position = position_dodge(1), size=4) + coord_polar() +
    labs(y = "Iron(mg)", title = "Iron(mg)", 
         subtitle = "Iron intake amount(mg) among each group of people")

fig_sodium <- ggplot(data_combined, aes(x = group_id, y = avg_sodium_, fill = diabetes, 
                                        shape = diabetes, color = diabetes)) +
    geom_bar(position = "dodge", stat = "identity") +
    geom_text(aes(label=round(avg_sodium_)), color = "black", 
              position = position_dodge(1), size=3) + coord_polar() +
    labs(y = "Sodium(mg)", title = "Sodium(mg)", 
         subtitle = "Sodium intake amount(mg) among each group of people")
fig_iron
fig_zinc
fig_sodium
```

So, fron the above 3 graphs, we can see that:  
(1) For the iron intake, we can see there exist some difference between diabetes groups. Especially for the male adults(19-40) who don't have health insurance, the diabetes group would obvious overload iron intakes in their meals.  
(2) For the zinc intake, according to the research paper, zinc can somewhat have beneficial effect on preventing diabetes problem. That can be supported by my plot that for those male teenagers(13-18) with health insurance, the non-diabetes group tend to get more zinc intakes in their meals.  
(3) For the sodium intake, people with more salt intake will tend to have a blood pressure problem and maybe relevant to diabetes problem. So from my plot we can see that for the male teenager group with health insurance, those have diabetes taking more amount of sodium than those without diabetes.

---

### Modeling
we are going to build penalized logistic model for our question. And we implement cross validation methods to get lambda parameter; parallel computing method to improve efficiency; AUC value to evaluate our model performance.
```{r model1, echo=FALSE}
## setup parallel computing parameter
ncores = 2  
# set up a cluster called 'cl'
cl = makeCluster(ncores)
# register the cluster
registerDoParallel(cl)

# set the number of testset partition times
N_test <- 10
# delete the insurance variable and add interactive term with intake variables
times_insur <- function(x) x * Data_final$insurance
Data_final <- select(Data_final, -weight) %>%
    mutate_at(c("iron", "calcium", "zinc", "sodium", "VA", "VC", "VE",
                "alcohol", "fat", "fiber", "sugar", "carbonhydrate",
                "energy", "protein"), times_insur) %>%
    right_join(Data_final, by = c("id", "survey_day", "gender", 
                                  "age_group", "income", "diabetes",
                                  "insurance"), suffix = c("_ins", "_ogn")) %>%
    select(-c("id", "weight", "insurance"))
```

```{r model2}
# get the number of obs
Nnum <- nrow(Data_final)
# the number of obs as test data
test_Num <- round(0.2 * Nnum)
# the number of obs as train and cv data
train_Num <- Nnum - test_Num

# building the model for N_test time in order to avoid overfitting problem
result = foreach(i = 1:N_test) %dopar% {
    library(glmnet)
    library(ROCit)
    # divide data into training set and test set
    set.seed(i)
    test_id <- sample(1:Nnum, test_Num)
    test_data <- Data_final[test_id,]
    train_cv_data <- Data_final[-test_id,]
    
    
    # divide the dependent variables and response in the training set
    x_train <- model.matrix(diabetes ~ ., train_cv_data)[, -1]
    y_train <- train_cv_data$diabetes    
    
    # use cross validation metthod to find the optimal lambda parameter
    cv.lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", 
                          type.measure = "auc", parallel = TRUE)
    
    # use the lambda which gives the best AUC value and avoid improving model 
    # complexity to build our model
    model_small <- glmnet(x_train, y_train, alpha = 1, famaily = "binomial", 
                          lambda = cv.lasso$lambda.min)
    
    # store the optimal lambda value
    lambda <- cv.lasso$lambda.min
    
    # make prediction using test data
    x_test <- model.matrix(diabetes ~ ., test_data)[, -1]
    y_test <- test_data$diabetes
    y_hat_small <- predict.glmnet(model_small, newx = x_test, type = "response")
    
    # evaluate model performance on test data using ROC curve and AUC value
    ROC_obj_small <- rocit(score = as.vector(y_hat_small), class = y_test)
    
    # store the AUC value and plot the ROC curve
    # plot(ROC_obj_small)
    AUC <- ROC_obj_small$AUC    
    c(lambda, AUC)
}  

# building final model for all of our data 
result <- matrix(unlist(result), ncol = N_test, nrow = 2)
x_all <- model.matrix(diabetes~., Data_final)[, -1]
y_all <- Data_final$diabetes

# choose the mean value of the optimal lambdas above as the final optimal lambda
# for our final model
lambda_final <- rowMeans(result)[1]
model <- glmnet(x_all, y_all, family = "binomial", alpha = 1, lambda = lambda_final)

# the coefficients of the final model
beta_all <- coef(model)

# estimation of the final model performance using the average AUC value above
AUC_final <- rowMeans(result)[2]
stopCluster(cl)
```

```{r show_result, echo=FALSE}
cat("Estimate model performance by AUC: ", AUC_final, "\n")
beta_all
ROC_obj <- rocit(score = as.vector(predict(model, x_all)), class = y_all)
plot(ROC_obj)
title("ROC curve for the final model on whole data")
```

### Model Interpretation
According to the final model we got above, we only remain 14 variables that are important chosen by lasso method.

And then we can see that:  
(1) For sugar predictor, the coefficients of the original one and the insurance interacted one are both negative, that means if a person has much sugar intake in his/her meal, then it may have less chance to be told that he/she has a diabetes issue.  
(2) For age predictor, the coefficients of the elder people are negative, that means an elder person may be more likely to be told having a diabetes issue.  
(3) For income predictor, people with higher income tend to have less probability to get diabetes issue. That make sense because rich people maybe pay more attention to their health status.  
(4) For insurance interacted term, there are only sugar and sodium interacted remain in the model. And that can answer our main question: 

People with health insurance are **more likely** to have diabetes issue with same amount **sodium, iron, fat** intakes than those without insurance;  
And they are **less likely** to have diabetes issue with same amoount **VC, VE, sugar, alcohol, protein** intakes than those without insurance.



## python

### data preprocess
```{python eval=FALSE}
#env set
import pandas as pd
#read DEMO_I.XPT
demo = pd.read_sas('/Users/weijiepan/Desktop/506_project/DEMO_I.XPT')
#select SEQN(id) RIDAGEYR(age) RIAGENDR(gender) INDFMIN2(Annual Family Income) 
#lower case 
demo = demo[["SEQN","RIAGENDR","RIDAGEYR","INDFMIN2"]]
demo = demo.rename(columns={"SEQN":"id","RIAGENDR": "gender", "RIDAGEYR": "age","INDFMIN2":"income"})
#income remove 12 13 and 14->12 15->13
demo = demo[((demo.income>=0) & (demo.income<=11)) | (demo.income==14) | (demo.income==15)]
demo.income[demo.income==14]=12
demo.income[demo.income==15]=13
#age divede 0ï½ž12 => 1 | 13ï½ž18 => 2 | 19ï½ž40 => 3 | 41ï½ž59 => 4 | 60ï½ž => 5 demo.age[(demo.age>=0) & (demo.age<=12)] = 1
demo.age[(demo.age>=13) & (demo.age<=18)] = 2
demo.age[(demo.age>=19) & (demo.age<=40)] = 3
demo.age[(demo.age>=41) & (demo.age<=59)] = 4
demo.age[(demo.age>=60)] = 5
#Diabetes
diabetes = pd.read_sas('/Users/weijiepan/Desktop/506_project/DIQ_I.XPT')
diabetes = diabetes[["SEQN","DIQ010"]]
diabetes = diabetes.rename(columns={"SEQN":"id","DIQ010":"diabetes"})
#change diabetes 2 -> No 1->Yes remove other 
diabetes=diabetes[(diabetes.diabetes==1) | (diabetes.diabetes==2)]
diabetes.diabetes[diabetes.diabetes==2]=0
#insurance
insurance = pd.read_sas('/Users/weijiepan/Desktop/506_project/HIQ_I.XPT')
insurance = insurance[["SEQN","HIQ011"]]
insurance = insurance.rename(columns={"SEQN":"id","HIQ011":"insurance"})
insurance = insurance[(insurance.insurance==1) | (insurance.insurance==2)]
insurance.insurance[insurance.insurance==2]=0
#day1 
#generate new col surveyday
#(df-df.mean())/df.std()
#drop day2 weight (WTDR2D)
day1 = pd.read_sas('/Users/weijiepan/Desktop/506_project/DR1TOT_I.XPT')
day1 = day1[["SEQN","WTDRD1","DR1TIRON","DR1TCALC","DR1TZINC","DR1TSODI","DR1TATOC","DR1TVARA",
             "DR1TALCO","DR1TVC","DR1TTFAT","DR1TFIBE","DR1TSUGR","DR1TCARB","DR1TKCAL","DR1TPROT"]]
day1 = day1.rename(columns={"SEQN":"id","WTDRD1":"weight",
                            "DR1TIRON":"iron","DR1TCALC":"calcium","DR1TZINC":"zine","DR1TSODI":"sodium",
                            "DR1TATOC":"VE","DR1TVARA":"VA","DR1TALCO":"alcohol","DR1TVC":"VC",
                            "DR1TTFAT":"fat","DR1TFIBE":"dietary fiber","DR1TSUGR":"sugar",
                            "DR1TCARB":"carbohydrate","DR1TKCAL":"energy","DR1TPROT":"protein"})
#(df-df.mean())/df.std()
#normalize
df=day1[["iron","calcium","zine","sodium","VE","VA","alcohol","VC",
     "fat","dietary fiber","sugar","carbohydrate","energy","protein"]]
day1[["iron","calcium","zine","sodium","VE","VA","alcohol","VC",
     "fat","dietary fiber","sugar","carbohydrate","energy","protein"]] = (df-df.mean())/df.std()
day1["surveyday"]=1
#day2 
#drop day1 weight (WTDRD1)
day2 = pd.read_sas('/Users/weijiepan/Desktop/506_project/DR2TOT_I.XPT')
day2 = day2[["SEQN","WTDR2D","DR2TIRON","DR2TCALC","DR2TZINC","DR2TSODI","DR2TATOC","DR2TVARA",
             "DR2TALCO","DR2TVC","DR2TTFAT","DR2TFIBE","DR2TSUGR","DR2TCARB","DR2TKCAL","DR2TPROT"]]
day2 = day2.rename(columns={"SEQN":"id","WTDR2D":"weight",
                            "DR2TIRON":"iron","DR2TCALC":"calcium","DR2TZINC":"zine","DR2TSODI":"sodium",
                            "DR2TATOC":"VE","DR2TVARA":"VA","DR2TALCO":"alcohol","DR2TVC":"VC",
                            "DR2TTFAT":"fat","DR2TFIBE":"dietary fiber","DR2TSUGR":"sugar",
                            "DR2TCARB":"carbohydrate","DR2TKCAL":"energy","DR2TPROT":"protein"})
#nomorlize
df=day2[["iron","calcium","zine","sodium","VE","VA","alcohol","VC",
     "fat","dietary fiber","sugar","carbohydrate","energy","protein"]]
day2[["iron","calcium","zine","sodium","VE","VA","alcohol","VC",
     "fat","dietary fiber","sugar","carbohydrate","energy","protein"]] = (df-df.mean())/df.std()
day2["surveyday"]=2
#demo -> demo + insurance
demo = demo.merge(insurance,left_on="id",right_on="id")
#demo -> diabetes
demo_diabetes = demo.merge(diabetes,left_on="id",right_on="id")
#day1 -> demo_diabetes
day1_demo_diabetes = day1.merge(demo_diabetes,left_on="id",right_on="id")
#day2 -> demo_diabetes
day2_demo_diabetes = day2.merge(demo_diabetes,left_on="id",right_on="id")
#vhat (day1_demo_diabetes,day2_demo_diabetes)
day12_demo_diabetes = pd.concat([day1_demo_diabetes,day2_demo_diabetes])
#drop nan
day12_demo_diabetes=day12_demo_diabetes.dropna(axis=0,how='any') 
day12_demo_diabetes[(day12_demo_diabetes.weight!=0)]

#after discussion decide no longer use the weight so drop it
day12_demo_diabetes = day12_demo_diabetes.drop(["weight"],axis=1)
```

### Data visualization of VA VC and VE

![](/Users/weijiepan/Documents/GitHub/STATS-506/data_visualization/py_Graphs/Vitamin_A.png)
![](/Users/weijiepan/Documents/GitHub/STATS-506/data_visualization/py_Graphs/Vitamin_C.png)
![](/Users/weijiepan/Documents/GitHub/STATS-506/data_visualization/py_Graphs/Vitamin_E.png)

As we can see from the above pictures, the intakes of vitamin A,C and E are almost the same in mostly group except for the group (teenage 13-18, male, with_ins) and the group (teenage 13-18, female, with_ins). It seems for me that vitamin A,C and E
do NOT play important roles in this model. Teenage with diabetes can not eat too much sugar, which push them to eat more fruits or vegetables. 
### model

```{python eval=FALSE}
import pandas as pd
#model part 
#read the data
data = pd.read_csv("/Users/weijiepan/Desktop/506_project/506_project_data.csv")
data = data.drop(["Unnamed: 0"],axis=1)


#create intecept terms
colnames=["iron","calcium","zine","sodium","VE","VA","alcohol","VC",
     "fat","dietary fiber","sugar","carbohydrate","energy","protein"]
for i in colnames:
    data[i+"_ins"] = data[i]*data["insurance"]


#divide it into X(preditors) y(result)
X = data.drop(["id","diabetes","insurance"],axis=1)
y = data["diabetes"]


#deal with categorical features
#age, gender, income
gender = pd.get_dummies(X['gender'],drop_first=True)
gender.columns=["gender"+str(i) for i in gender.columns]

age = pd.get_dummies(X['age'],drop_first=True)
age.columns=["age"+str(i) for i in age.columns]

income = pd.get_dummies(X['income'],drop_first=True)
income.columns=["income"+str(i) for i in income.columns]

X.drop(['gender','age','income'],axis=1,inplace=True)

X = pd.concat([X,gender,age,income],axis=1)

#split it into training set and test set
from sklearn import model_selection
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, 
                                                                    test_size=0.2,
                                                                    random_state=42,
                                                                    stratify=y)
                                                                    
#build the model
from sklearn.metrics import accuracy_score 
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegressionCV

model = LogisticRegressionCV(penalty='l1',scoring='roc_auc',solver="saga",
                             cv=10,class_weight={1:.9,0:.1},max_iter=1000)
model.fit(X_train,y_train)

import matplotlib.pyplot as plt
#display the coef (sorted)
coef = pd.Series(model.coef_[0,:], index = X_train.columns)
plt.rcParams['figure.figsize'] = (8.0, 10.0)
coef.sort_values().plot(kind="barh")
```
![](/Users/weijiepan/Desktop/506_project/pic/coef_sorted.png)


```{python eval=FALSE}
from sklearn.metrics import roc_curve
#plot auc curve 
ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs = model.predict_proba(X_test)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]


# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
```

```{python echo=FALSE}
print('''No Skill: ROC AUC=0.500
Logistic: ROC AUC=0.849
''')
```

```{python eval=FALSE}
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()
```

![](/Users/weijiepan/Desktop/506_project/pic/roc_curve.png)

### Model Interpretation
According to the final model we got above, we only remain 25 variables that are important chosen by lasso method.

As we can see form the coef_sorted picture: 
(1) The age plays an important role in the model. As people become older, they may take a greater risk to have a diabete. What interesting is that the income play a tricky role in this model. We can see at first when income increase, people have more risk to have a diabete. But after that as the imcome increase, people have lower risk to have a diabetes.   
(2) For sugar predictor, the coefficients of the original one and the insurance interacted one are both negative, that means if a person has much sugar intake in his/her meal, then it may have less chance to be told that he/she has a diabetes issue.   
(3) From the interaction terms, we can see sodium, iro, and fat are positive. And VC alcohol and VE are negative.

People with health insurance are **more likely** to have diabetes issue with same amount **sodium, iron, fat** intakes than those without insurance;  
And they are **less likely** to have diabetes issue with same amoount **VE, alcohol, VA** intakes than those without insurance.




## STATA

### Heading 1

### Heading 2

### Heading 3

### Heading 4

### Results

# Summary

# Discussion
During Cross-Validation method of picking the optimal lambda,
programming done in Python and R used the AUC to choose 
the optimal lambda whereas programming done in STATA used
the lambda that minimizes the loss measure. This is due to a limitation in STATA
that was encountered.

# References
STATA:

Luque-Fernandez MA, Redondo-Sanchez D, Maringe C. (2019). Cross-validated Area Under the Curve. GitHub repository, https://github.com/migariane/cvauroc

Ahrens, A., Hansen, C.B., Schaffer, M.E. 2019.  lassologit: Stata module for logistic lasso regression.
        http://ideas.repec.org/c/boc/bocode/XXXXX.html
